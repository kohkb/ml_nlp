{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 実装方針\n",
    "#### 1. タグ候補を出す\n",
    "形態素解析で「名詞」かつ「一般」の単語を取り出す。この時、出現回数が多いものと1つしかないものは取り除く\n",
    "\n",
    "#### 2. 全文書からタグ候補以外を削除する\n",
    "タグを付ける上ではノイズにしかならないので取り除く\n",
    "\n",
    "#### 3. タグ候補を取り除いたもの同士で、類似文書を取得する\n",
    "文書をTF-IDFでベクトル化し、コサイン類似度を計算する。\n",
    "類似している文章をそれぞれ5つずつ取り出す\n",
    "\n",
    "#### 4. 類似している文章を全て足し合わせ、そこで使用回数が多いものをタグとする\n",
    "足し合わせることで下記のメリットがある\n",
    "- 表記揺れに強くなる\n",
    "- 文書に存在していない単語のタグ付けができる\n",
    "\n",
    "例えば１つの文書だけでタグ付けを判断してしまうと、「車」というタグがつくものと、「自動車」というタグがつくものにわかれてしまう可能性があるが、似た文書の和を先にとっておくことで、どちらのタグもつけることができる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリ読み込み\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "import MeCab\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# CSV読み込み\n",
    "path=\"csv/\"\n",
    "train = pd.read_csv(path + 'train.csv')\n",
    "\n",
    "# MeCab\n",
    "m = MeCab.Tagger (\"-Ochasen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [実験]MeCab 「名詞」の「一般」のみ取得する\n",
    "print(m.parse(\"天皇陛下の即位と同時に元号が平成から「令和」に改まった\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 名詞かつ一般を抽出する関数\n",
    "def extract_nouns(text):\n",
    "    node = m.parseToNode(text)\n",
    "    nouns = []\n",
    "     \n",
    "    while node:\n",
    "        features =  node.feature.split(',')\n",
    "         \n",
    "        if features[0] == '名詞' and features[1] == '一般':\n",
    "            nouns.append(node.surface)\n",
    "         \n",
    "        node = node.next\n",
    "     \n",
    "    return set(nouns)\n",
    "\n",
    "# titleも含めて名詞を抽出する\n",
    "train['text'] = train['title'] + train['content']\n",
    "train['words'] = train['text'].apply(lambda x: extract_nouns(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単語の出現頻度確認\n",
    "from collections import defaultdict\n",
    "\n",
    "unigrams = defaultdict(int)\n",
    "\n",
    "for row in train['words']:\n",
    "    for word in row:  \n",
    "      unigrams[word] += 1\n",
    "    \n",
    "df_unigrams = pd.DataFrame(sorted(unigrams.items(), key=lambda x: x[1])[::-1])\n",
    "df_unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単語の出現頻度確認（グラフ）\n",
    "import matplotlib\n",
    "\n",
    "font = {\"family\": \"TakaoGothic\"}\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "igfont = {'family':'IPAexGothic'}\n",
    "plt.title('出現回数比率（1~100）', **igfont)\n",
    "plt.hist(df_unigrams[1], range=(1,100), bins=16, rwidth=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  名詞かつ一般でないものは取り除いて分かち書きにする\n",
    "def wakati(text):\n",
    "    node = m.parseToNode(text)\n",
    "    wakati_content = ''\n",
    "    \n",
    "    while node:\n",
    "        features =  node.feature.split(',')\n",
    "        \n",
    "        if features[0] == '名詞' and features[1] == '一般':\n",
    "          wakati_content = wakati_content + node.surface + ' '\n",
    "        \n",
    "        node = node.next\n",
    "    \n",
    "    return wakati_content\n",
    "\n",
    "train['wakati'] = train['text'].apply(lambda x: wakati(str(x)))\n",
    "train['wakati']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 出現回数は1が圧倒的に多い。1回だとタグ付に適さないので取り除く\n",
    "# 出現回数が多いものについても、タグとして適さないと考えられるので取り除く\n",
    "# ここでは出現回数が50回以下かつ2回以上をタグ候補として残す\n",
    "df_tags = pd.DataFrame(df_unigrams.loc[df_unigrams[1] <= 50])\n",
    "df_tags = pd.DataFrame(df_tags.loc[df_unigrams[1] >= 2])\n",
    "\n",
    "np_tags = np.array(df_tags[0])\n",
    "np_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# タグ候補の単語のみ各文章に残す\n",
    "# 実行時間は30秒ほど\n",
    "def remove_unselectable_tags(text):\n",
    "    train_words = text.split()\n",
    "    filtered_text = ''\n",
    "    \n",
    "    for word in train_words:\n",
    "      for tag_word in np_tags:\n",
    "        if word == tag_word:\n",
    "            filtered_text =  filtered_text + word + ' '\n",
    "    \n",
    "    return filtered_text\n",
    "\n",
    "\n",
    "train['wakati'] = train['wakati'].apply(lambda x: remove_unselectable_tags(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリ読み込み\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "transformer = TfidfTransformer()\n",
    "tf = vectorizer.fit_transform(train['wakati']) \n",
    "tfidf = transformer.fit_transform(tf) \n",
    "tfidf_array = tfidf.toarray()\n",
    "cs = cosine_similarity(tfidf_array, tfidf_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDFからのコサイン類似度で文書をクラスタリング\n",
    "docs = train['wakati']\n",
    "\n",
    "indexs = []\n",
    "for (i, c) in enumerate(cs):\n",
    "    c_np = np.array(c)\n",
    "    c_sort = np.sort(c_np)\n",
    "    index1 = np.where(c_np == c_sort[-2])[0][0]\n",
    "    index2 = np.where(c_np == c_sort[-3])[0][0]\n",
    "    index3 = np.where(c_np == c_sort[-4])[0][0]\n",
    "    index4 = np.where(c_np == c_sort[-5])[0][0]\n",
    "    index5 = np.where(c_np == c_sort[-6])[0][0]\n",
    "    indexs.append([\n",
    "                   i,\n",
    "                   '\"' + docs[i] + '\"',\n",
    "                   '\"' + docs[index1] + '\"',\n",
    "                   '\"' + docs[index2] + '\"',\n",
    "                   '\"' + docs[index3] + '\"',\n",
    "                   '\"' + docs[index4] + '\"',\n",
    "                   '\"' + docs[index5] + '\"',\n",
    "                   c_sort[-2], c_sort[-3], c_sort[-4], c_sort[-5], c_sort[-6]\n",
    "                   ])\n",
    "\n",
    "df_result = pd.DataFrame(indexs, columns=['id', 'target', 'top1', 'top2', 'top3', 'top4', 'top5', 'score1', 'score2', 'score3', 'score4', 'score5'])\n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用頻度が高い言葉を取り出す関数\n",
    "def count_tags(text):\n",
    "    contents = text.split()\n",
    "    \n",
    "    tag_counts = defaultdict(int)\n",
    "\n",
    "    for word in contents:\n",
    "      tag_counts[word] += 1\n",
    "    \n",
    "\n",
    "    tag_counts = sorted(tag_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # タグを5つままで出力する。ただしcountが閾値を超えているもののみとする\n",
    "    threshold = 5\n",
    "    tags = []\n",
    "    cnt = 0\n",
    "    for i in range(len(tag_counts)):\n",
    "        if cnt > 4:\n",
    "            break\n",
    "        if tag_counts[i][1] >= threshold:\n",
    "            tags.append(tag_counts[i][0])\n",
    "            cnt += 1\n",
    "            \n",
    "\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 類似している5つの文書を足し合わせ、使用頻度が高い上位5つの言葉をタグとする\n",
    "df_result['candidates'] = df_result['target']+df_result['top1']+df_result['top2']+df_result['top3'] + df_result['top4'] + df_result['top5']\n",
    "df_result['tags' ] = df_result['candidates'].apply(lambda x: count_tags(str(x)))\n",
    "df_result['tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 提出用データ生成\n",
    "df_sub = pd.read_csv(path + 'train.csv')\n",
    "\n",
    "def get_tag_by_index(tags, index):    \n",
    "    if len(tags) < index:\n",
    "        return ''\n",
    "    else:\n",
    "        return tags[index-1]\n",
    "    \n",
    "df_sub['tag1'] = df_result['tags'].apply(lambda x: get_tag_by_index(x,1))\n",
    "df_sub['tag2'] = df_result['tags'].apply(lambda x: get_tag_by_index(x,2))\n",
    "df_sub['tag3'] = df_result['tags'].apply(lambda x: get_tag_by_index(x,3))\n",
    "df_sub['tag4'] = df_result['tags'].apply(lambda x: get_tag_by_index(x,4))\n",
    "df_sub['tag5'] = df_result['tags'].apply(lambda x: get_tag_by_index(x,5))\n",
    "df_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub.to_csv(\"csv/submission.csv\", encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 今後の課題\n",
    "#### 1. パラメータの最適化\n",
    "今回パラメータが出てくるのは下記3箇所\n",
    "- タグ候補 \n",
    "- 似た文書をいくつ足すか\n",
    "- タグを最終的に決定するための閾値\n",
    "\n",
    "現在は感覚で各値を設定しているが、もうちょっといい方法がないか検討したい\n",
    "\n",
    "#### 2. 別の形態素解析器を使う\n",
    "「暗号通貨」という言葉が「暗号」と「通貨」　のようにわかれてしまっているのでそこを改善したい\n",
    "\n",
    "#### 3. 別の手法での表記ゆれ対応\n",
    "今回は類似文章の和を取ることで、表記ゆれに対応したが、LSI(潜在意味解析)というのものも使ってみたい"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
